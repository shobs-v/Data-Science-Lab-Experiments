{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Name: Shobhit Verma<br>\nClass: BE COMPS<br>\nBatch: D<br>\nRoll No: 33<br>\nUID: 2018130062<br>\n\n# Experiment - 6","metadata":{}},{"cell_type":"markdown","source":"# Contents table","metadata":{}},{"cell_type":"markdown","source":"- [0. Preamble](#0.)\n    - [0.1 Description](#0.1)\n    - [0.2 Imports](#0.2)\n- [1. Importing and visualising](#1.)\n    - [1.1 Importing data](#1.1)\n    - [1.2 Clenaing data](#1.2)\n    - [1.3 Visualising data](#1.3)\n- [2. Statistics](#2.)\n    - [2.1 Percentage Change](#2.1)\n    - [2.2 Returns](#2.2)\n    - [2.3 Comparing Time Series](#2.3)\n- [3. Decomposing Data](#3.)\n    - [3.1 Main Components](#3.1)\n- [4. Forecasting with an LSTM](#4.)\n    - [4.1 Preparing Data](#4.1)\n    - [4.2 Modelling](#4.2)\n    - [4.3 Results](#4.3)","metadata":{}},{"cell_type":"markdown","source":"# <a id='0.'> 0. Preamble </a>","metadata":{}},{"cell_type":"markdown","source":"### <a id='0.1'> 0.1 Description </a>","metadata":{}},{"cell_type":"markdown","source":"## Aim","metadata":{}},{"cell_type":"markdown","source":"In this notebook I aim to analyse the historical stock data for FTSE, S&P, DAX, and NIKKEI stock - from 1994-2018. The notebook includes visualisations, comparisons, and forecasting, both naively and using a LSTM (an RNN based) neural network. <br> <br>For the forecasting I will be focusing on the SPX stock","metadata":{}},{"cell_type":"markdown","source":"## Data Description","metadata":{}},{"cell_type":"markdown","source":"##### All values are the price of the stock at close time, in dollars $\n\n- 'spx' is the S&P 500 gauge of U.S Equities\n- 'dax' is a total return index of 30 selected German blue chip stocks\n- 'ftse' is a share index of the top 100 companies listed on the London Stock Exchange\n- 'nikkei' is a price-weighted average of 225 top-rated Japanese companies listed on Tokyo Stock Exchange\n\n","metadata":{}},{"cell_type":"markdown","source":"### <a id ='0.2'> 0.2 Imports </a>","metadata":{}},{"cell_type":"code","source":"#Importing stuff\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\nwarnings.simplefilter('ignore', ConvergenceWarning)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import pylab\nimport sklearn.metrics\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n\nimport seaborn as sns\n%pylab inline\nsns.set(style=\"darkgrid\") #Comment out if pyplot style is wanted\n\nimport datetime as dt\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as L\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport os\nprint(f\"filename: {os.listdir('../input/financial-markets')}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Class to help with formatting printed text\nclass color:\n   BOLD = '\\033[1m'\n   END = '\\033[0m'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='1.'> 1. Importing and visualising </a>","metadata":{}},{"cell_type":"markdown","source":"### <a id ='1.1'> 1.1 Importing data </a>","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/financial-markets/Index2018.csv',\n                   parse_dates=['date'],\n                   date_parser = (lambda date: dt.datetime.strptime(date,'%d/%m/%Y'))\n                   ,index_col='date')      \n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id = '1.2'> 1.2 Cleaning Data </a>","metadata":{}},{"cell_type":"code","source":"print(color.BOLD + 'Description of data:' + color.END)\nprint(data.describe())\n\nprint(color.BOLD + '\\nNumber of Null values:' + color.END)\ndata.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see we don't actually have any na values. We've already formatted 'Date' and made it the index. This means we have no data cleaning to do!","metadata":{}},{"cell_type":"markdown","source":"### <a id='1.3'> 1.3 Visualising Data </a>","metadata":{}},{"cell_type":"code","source":"sns.set(font_scale=(1.5))\ndata.plot(subplots=True, figsize=(15,12),ylabel =('Close Value'),\n                         title = ['spx from 1994-2018','dax from 1994-2018',\n                                  'ftse from 1994-2018','nikkei from 1994-2018',])\nplt.savefig('stocks.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We will be focusing on the ftse and spx stocks","metadata":{}},{"cell_type":"markdown","source":"# <a id='2.'> 2. Statistics </a>","metadata":{}},{"cell_type":"markdown","source":"## <a id='2.1'> 2.1 Percentage Change </a>","metadata":{}},{"cell_type":"code","source":"sns.set(font_scale=2.0)\ndata[['ftse','spx']].pct_change().multiply(100).plot(subplots=True,figsize=(20,8),fontsize=20,ylabel='Gain (%)')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id =2.2> 2.2 Returns </a>","metadata":{}},{"cell_type":"markdown","source":"#### Note the different scales!","metadata":{}},{"cell_type":"code","source":"sns.set(font_scale=(1.4))\ndata['ftse_return'] = data.ftse.diff() \ndata['spx_return'] = data.spx.diff()\ndata['dax_return'] = data.dax.diff()\ndata['nikkei_return'] = data.nikkei.diff()\ndata[['ftse_return',\n'spx_return',\n'dax_return',\n'nikkei_return']].plot(subplots=True,figsize=(15,15),fontsize=20,ylabel='Absolute gain',legend=True)\n\nprint(color.BOLD + 'Absolute returns for all 4 stocks' + color.END)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Of course, a higher absolute gain does not necessarily indicate a 'better' stock. Let us now compare the growth of the stocks","metadata":{}},{"cell_type":"markdown","source":"## <a id = '2.3'> 2.3 Comparing Time Series </a>","metadata":{}},{"cell_type":"markdown","source":"In order to effectively compare the time series we need to format the data, so the stocks start at the same value <br>\nTo do this we will divide each series by its respective starting value, and multiply by 100 for more similarity with original","metadata":{}},{"cell_type":"code","source":"format_ftse = data.ftse.divide(data.ftse.iloc[0]).multiply(100)\nformat_spx = data.spx.divide(data.spx.iloc[0]).multiply(100)\nformat_nikkei = data.nikkei.divide(data.nikkei.iloc[0]).multiply(100)\nformat_dax = data.dax.divide(data.dax.iloc[0]).multiply(100)\n\nsns.set(font_scale=(1.9))\nformat_ftse.plot(fontsize=20,ylabel='Relative value',legend=True)\nformat_spx.plot(fontsize=20,ylabel='Relative value',legend=True)\nformat_dax.plot(fontsize=20,ylabel='Relative value',legend=True)\nformat_nikkei.plot(figsize=(18,8),fontsize=20,ylabel='Relative value',legend=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that spx and dax have significantly outperformed the ftse & nikkei, in terms of value gain.","metadata":{}},{"cell_type":"markdown","source":"# <a id=3.> 3. Decomposing data</a>","metadata":{}},{"cell_type":"markdown","source":"## <a i='3.1'> 3.1 Main components </a>","metadata":{}},{"cell_type":"markdown","source":"A time series such at these, can be decomposed into 3 main building blocks: <br>\n- Trend - General direction of data\n- Seasonality - trends which repeat after a given interval\n- Noise - 'random' fluctuations in values\n\nWe will break down the SPX into these 3 components","metadata":{}},{"cell_type":"code","source":"#First lets view just SPX\nsns.set(font_scale=(1.3))\ndata.spx.plot(figsize=(10,6),fontsize=15,ylabel='Close Value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We will use statsmodels' seasonal decomposer (multiplicitive)\n#So, ftse[t] = Trend[t] * Seasonal[t] * Noise[t]\n\nimport statsmodels.api as sm\nfrom pylab import rcParams\n\nrcParams['figure.figsize'] = 11, 9\nspx_decomposed = sm.tsa.seasonal_decompose(data.spx,period=400,model='multiplicative') #yearly seasonality\n\nspx_decomposed.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generally an upward trend in the plot, though there is clearly exceptions (dot.com crash of 2000 and the american financial crisis of sept 2008)\n\nWe also see the seasonality is not particularly strong - making predicitons by hand harder\n\n","metadata":{}},{"cell_type":"markdown","source":"## <a id='4.'> 4. Forecasting with a LSTM </a>","metadata":{}},{"cell_type":"markdown","source":"Here I will train a LSTM based model using tensorflow and keras to try and predict the future values   \nI'll split the data into train and validation, in order to quantify the accuracy of the model","metadata":{}},{"cell_type":"markdown","source":"## <a id='4.1'> 4.1 Preparing Data </a>","metadata":{}},{"cell_type":"markdown","source":"First I will make a numpy array from the relevant data","metadata":{}},{"cell_type":"code","source":"dataset = data.spx #this is the stock data we want\ndataset = np.array(dataset.values) #turning into an array\n\nprint(color.BOLD + 'dataset peak:\\n' + color.END,dataset)\n\n#getting the length of training data, with a 80/20 split (ish, not quite divisible perfectly)\ntraining_len = int(len(dataset)*0.80//1 -15)\n\ntest_len = int(len(dataset) - training_len)\n\nprint(('\\ntraining length: {}\\ntest length: {}').format(training_len,test_len))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should scale the train&test data before hand, to avoid problems such as exploding or vanishing gradient","metadata":{}},{"cell_type":"code","source":"#Applying scaling using Sklearn's MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1)) \nscaled_data = scaler.fit_transform(dataset.reshape(-1, 1))\n\n#Now splitting data into train and test\ntrain_data = scaled_data[:training_len,:]\ntest_data = scaled_data[training_len-50:,:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we can pass the data into an Neural Network, we need to make sure the data is windowed, so the model can use historic data from periods longer than 1","metadata":{}},{"cell_type":"markdown","source":"#### Windowing data","metadata":{}},{"cell_type":"code","source":"x_train = []\ny_train = []\nx_test = []\nwindow_size = 50\n\nfor i in range(window_size, len(train_data)):\n    x_train.append(train_data[i-window_size:i, 0])\n    y_train.append(train_data[i, 0])\n    \n\nfor i in range(window_size, len(test_data)):\n    x_test.append(test_data[i-window_size:i, 0])\n            \n#Converting into numpy arrays to feed in model  \nx_train = np.array(x_train)\ny_train = np.array(y_train)\nx_test = np.array(x_test)\n\n#data must be in a 3d array - as we have  amount of data points, window_size, and batch size\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1)) \nx_test = np.reshape(x_test,(x_test.shape[0], x_test.shape[1] , 1))\nprint('x_train shape: ', np.shape(x_train))\nprint('x_test_shape', np.shape(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='4.2'> 4.2 Modelling </a>","metadata":{}},{"cell_type":"code","source":"#Build the LSTM model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Conv1D , Dropout #Conv1d and Dropout didn't improve this model, so have been left out\n\nmodel = Sequential()\nmodel.add(LSTM(50, return_sequences=True,input_shape=(x_train.shape[1],1)))\nmodel.add(LSTM(50, return_sequences= False))\nmodel.add(Dense(25,activation='relu'))\nmodel.add(Dense(1))\n\n# Compile the model\nmodel.compile(optimizer='Adam', loss='mean_squared_error')\n\n#Train the model\nmodel.fit(x_train, y_train, batch_size=1, epochs=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(x_test) #obtaining the models predictions\npredictions = scaler.inverse_transform(predictions) #Obtaining actual predictions from the normalised predictions\npredictions = predictions[:,0]\nprint(predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='4.3'> 4.3 Results </a>","metadata":{}},{"cell_type":"code","source":"train = data.spx[:training_len]\nvalidation = pd.DataFrame(data.spx[training_len:])\nvalidation['Predictions'] = predictions\n\nsns.set(font_scale=1.5)\n\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(16,8))\nax2.plot(train)\nax2.plot(validation[['spx','Predictions']])\nax2.legend(['Train', 'Validation', 'Predictions'], loc='lower right')\nax2.set_xlabel('Date', fontsize=16)\nax2.set_ylabel('Close Price USD ($)', fontsize=16)\n\n\nax1.plot(validation['spx'],color='darkorange')\nax1.plot(validation['Predictions'],color='green')\nax1.legend(['Validation','Predictions'],loc='lower right')\nax1.set_xlabel('Date', fontsize=16)\nax1.set_ylabel('Close Price USD ($)', fontsize=16)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model seems to work very well for the validation period! Let us compare it to a naive forecast, which simply projects the previous days value to the next day\n\nWe also have an issue in that we don't particularly want to use data from 20 years ago to help train our model as it's not very relevant, but if we only use data from, say, the last 10 years, our model will be terrible at predicting any downturns. This is of course one of the big difficulties with stock data","metadata":{}},{"cell_type":"code","source":"#'shifting' the data by 1, to get the naive forecasting, and duplicating the previous value to preserve lengths\n\nnaive = np.append(validation.spx.values[1:],validation.spx.values[-1])\n\n#plotting\nplt.plot(naive-validation.spx.values,label = 'naive')\nplt.plot(validation.spx.values-predictions,color='green',label = 'lstm predictions')\nplt.legend(fontsize=14)\nplt.ylabel('error')\nplt.xlabel('data point',fontsize=13)\n\nprint('MAE of predictions:',sklearn.metrics.mean_absolute_error(validation.spx.values,predictions))\nprint('MSE of predictions:',sklearn.metrics.mean_squared_error(validation.spx.values,predictions)**0.5)\nprint('\\nMSE of naive-forecast:',np.mean(np.square((validation.spx.values-naive)))**0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although these results look promising on first inspection (as only 1 epoch trained on), after comparing with a naive forecast we can see that the two models do very similar things. This is explained as in real stock prices have so many variables which can't really be accounted for in a model such as this. Hence, the optimal solution for the LSTM is not too disimilar from simply a random walk , which trails by 1 day.\n\nIf this wasn't so, there would be a massive amount of millionaires trading just using LSTM's!","metadata":{}}]}